<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Lingzhi Zhang</title>
  
  <meta name="author" content="Lingzhi Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lingzhi Zhang</name>
              </p> 
                  I am a Research Scientist at Adobe and Tech Lead for multiple generations of Generative Remove features 
                  deployed in Photoshop and Lightroom ecosystems (desktop, mobile, web, etc.). I am interested in multimodal 
                  media understanding and synthesis. 
                  <br>
                  <br>
                  I received my PhD in Computer and Information Science at the University of Pennsylvania, advised by
                <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a>.
              </p>
              <p style="text-align:center">
                <a href="https://github.com/owenzlz">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=d85t3QEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/lingzhi-zhang-30344785/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zlz.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/zlz.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px 20px 5px 20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:5px 20px 20px 20px;width:100%;vertical-align:middle">
              <ul>
                <li><strong>[10/2025]</strong> - Presented <a href="https://www.adobe.com/max/2025/sessions/project-trace-erase-gs3-7.html">"Project Trace Erase"</a> (Next-Gen Removal Tech) at Adobe MAX Sneaks 2025 <img src="images/MAX_icon.png" style="height:1em;vertical-align:middle;" alt="MAX">. (<a href="https://www.youtube.com/watch?v=kFKTIkxI0Qw&vl=en">Youtube</a>). </li>
                <li><strong>[10/2025]</strong> - Media coverages of "Project Trace Erase": <a href="https://www.cnet.com/tech/services-and-software/adobes-ai-makes-it-quicker-than-ever-to-edit-your-photos-i-got-an-early-look/">CNET</a>, <a href="https://www.cgchannel.com/2025/10/sneak-peeks-adobes-project-surface-swap-and-project-trace-erase/">CG Channel</a>, <a href="https://photofocus.com/news/adobe-max-sneaks-2025/">PhotoFocus</a>, <a href="https://cybernews.com/ai-tools/adobe-max-2025-overview/">Cybernews</a>, <a href="https://currently.att.yahoo.com/att/live-adobe-max-sneak-peek-024200289.html">Yahoo</a>, <a href="https://www.capturemag.com.au/news/adobe-sneaks-reveal-mind-blowing-3d-ai-photo-alterations-and-radical-transformations-in-video-editing-software">Capture Magazine</a>, <a href="https://www.digitalcameraworld.com/tech/software/i-just-got-a-glimpse-at-the-future-of-photoshop-adobe-teases-tools-for-relighting-photos-creating-composites-and-swapping-surfaces">Digital Camera World</a>, <a href="https://biz.chosun.com/en/en-it/2025/10/30/ZKTQ4VKG4BG3PGFE3U4QHPZUYI/">The Chosun Biz</a>, <a href="https://www.business-standard.com/technology/tech-news/adobe-s-new-ai-tools-can-erase-objects-from-videos-turn-lamps-on-magically-125110300351_1.html">Business Standard</a>, <a href="https://www.redsharknews.com/adobe-max-2025-sneaks-first-look">RedShark News</a>, <a href="https://www.techradar.com/pro/better-object-removal-improved-lighting-and-more-these-sneaks-could-be-adobes-next-big-leap-forward">TechRadar</a>. </li>
                <li><strong>[10/2025]</strong> - Shipped an upgraded, ultraâ€“high-quality removal model in Photoshop (Beta) <img src="images/Ps_beta_icon.png" style="height:1.2em;vertical-align:middle;" alt="Ps Beta">, supporting native 2K-resolution generation and significantly improved visual realism. </li>
                <li><strong>[08/2025]</strong> - Extended the ultra-fast (~1s) diffusion-based removal model to Lightroom <img src="images/Lr_icon.png" style="height:1.2em;vertical-align:middle;" alt="Lr"> and Adobe Camera Raw <img src="images/Lrc_icon.png" style="height:1.2em;vertical-align:middle;" alt="ACR">, with full compatibility and high fidelity on RAW images. </li>
                <li><strong>[08/2025]</strong> - <a href="https://arxiv.org/pdf/2510.06215">"Fine-grained Defocus Blur Control for Generative Image Models"</a> has been accepted to WACV 2025</li>
                <li><strong>[06/2025]</strong> - Shipped an ultra-fast (~1s) diffusion-based removal model in Photoshop <img src="images/Ps_icon.png" style="height:1.2em;vertical-align:middle;" alt="Ps">, delivering state-of-the-art production-level quality with innovative modeling. </li>
                <li><strong>[06/2025]</strong> - <a href="https://arxiv.org/pdf/2506.05342">"Refer to Anything with Vision-Language Prompts"</a> has been accepted to ICCV 2025 </li>
                <li><strong>[05/2025]</strong> - <a href="https://aclanthology.org/2025.findings-acl.1318.pdf">"Cautious Next Token Prediction"</a> has been accepted to ACL 2025 Findings</li>
                <li><strong>[04/2025]</strong> - <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/You_Layer-_and_Timestep-Adaptive_Differentiable_Token_Compression_Ratios_for_Efficient_Diffusion_CVPR_2025_paper.pdf">"Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers"</a> has been accepted to CVPR 2025</li>
                <li><strong>[04/2025]</strong> - <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Hua_FINECAPTION_Compositional_Image_Captioning_Focusing_on_Wherever_You_Want_at_CVPR_2025_paper.pdf">"FineCaption: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity"</a> has been accepted to CVPR 2025</li>
                <li><strong>[08/2024]</strong> - <a href="https://ieeexplore.ieee.org/abstract/document/10943986/">"Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models"</a> has been accepted to WACV 2024</li>
                <li><strong>[08/2024]</strong> - <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Xu_Detecting_Origin_Attribution_for_Text-to-Image_Diffusion_Models_WACV_2025_paper.pdf">"Detecting Origin Attribution for Text-to-Image Diffusion Models"</a> has been accepted to WACV 2024</li>
                <li><strong>[04/2024]</strong> - <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chiu_Brush2Prompt_Contextual_Prompt_Generator_for_Object_Inpainting_CVPR_2024_paper.pdf">"Brush2Prompt: Contextual Prompt Generator for Object Inpainting"</a> has been accepted to CVPR 2024</li>
                <li><strong>[04/2024]</strong> - <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.pdf">"Amodal Completion via Progressive Mixed Context Diffusion"</a> has been accepted to CVPR 2024</li>
                <li><strong>[02/2024]</strong> - My PhD research paper <a href="https://arxiv.org/pdf/2208.03357">"Perceptual Artifacts Localization"</a> was integrated in Photoshop <img src="images/Ps_icon.png" style="height:1.2em;vertical-align:middle;" alt="Ps">. </li>
                <li><strong>[08/2023]</strong> - Defended my PhD thesis at University of Pennsylvania <img src="images/Penn_icon.png" style="height:1.2em;vertical-align:middle;" alt="Penn">!</li>
                <li><strong>[04/2023]</strong> - My PhD research paper <a href="https://arxiv.org/pdf/2208.03552">"Guided PatchMatch"</a> was productized and shipped in Photoshop <img src="images/Ps_icon.png" style="height:1.2em;vertical-align:middle;" alt="Ps">. The first hybrid deep-learning and patch-synthesis method that can generate images at unlimited ultra-high resolution.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="PAL4VST()" onmouseover="PAL4VST()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/pal4vst.png' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2310.05590.pdf">
                <papertitle>Perceptual Artifacts Localization for Image Synthesis Tasks</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Zhengjie Xu,
              Connelly Barnes, 
              Yuqian Zhou,
              Qing Liu, 
              He Zhang,
              Zhe Lin,
              Eli Shechtman, 
              Sohrab Amirghodsi,
              Jianbo Shi
              <br>
              <em>ICCV 2023</em>
              <br>
              <a href="https://arxiv.org/pdf/2310.05590.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/PAL4VST">code</a> / 
              <a href="https://github.com/owenzlz/PAL4VST">dataset</a>
              <p></p>
              <p>
                We generalize Perceptual Artifacts Localization to ten diverse image synthesis, and shows promising accuracy. 
                We also show the effectiveness of automatic artifacts fixing and quality assessment as downstream applications. 
              </p>
            </td>
          </tr>

          <tr onmouseout="InpaintArtifacts()" onmouseover="InpaintArtifacts()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/PAL4Inpaint.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03357.pdf">
                <papertitle>Perceptual Artifacts Localization for Inpainting</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Yuqian Zhou,
              Connelly Barnes,
              Sohrab Amirghodsi,
              Zhe Lin,
              Eli Shechtman, 
              Jianbo Shi
              <br>
              <em>ECCV 2022 (Oral)</em>
              <br>
              <a href="https://arxiv.org/pdf/2208.03357.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/PAL4Inpaint">code</a> / 
              <a href="https://github.com/owenzlz/PAL4Inpaint">dataset</a>
              <p></p>
              <p>
                We formulate a new task of learning perceptual artifacts localization on the inpainted images.
                We propose a high-quality labeled dataset, successuflly train a model to localize the perceptual 
                artifacts, and demonstrate downstream practical applications to no-reference single image quality 
                assessment and iterative fill. 
              </p>
            </td>
          </tr>
 
          <tr onmouseout="SuperCAF()" onmouseover="SuperCAF()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/SuperCAF.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03552.pdf">
                <papertitle>Inpainting at Modern Camera Resolution by Guided PatchMatch with Auto-Curation</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Connelly Barnes,
              Kevin Wampler,
              Sohrab Amirghodsi,
              Eli Shechtman, 
              Zhe Lin, 
              Jianbo Shi
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://arxiv.org/pdf/2208.03552.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/SuperCAF">code</a> / 
              <a href="https://github.com/owenzlz/SuperCAF">dataset</a>
              <p></p>
              <p>
                We propose a hybrid deep learning and patch-based approach for inpainting at modern camera resolution (4K+).
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="EgoHOS()" onmouseover="EgoHOS()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/egohos.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03826.pdf">
                <papertitle>Fine-Grained Egocentric Hand-Object Segmentation: Dataset, Model, and Applications</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Shenghao Zhou*,
              Simon Stent,
              Jianbo Shi (* equal contribution)
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://www.seas.upenn.edu/~shzhou2/projects/eos_dataset/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.03826.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/EgoHOS">code</a> / 
              <a href="https://github.com/owenzlz/EgoHOS">dataset</a>
              <p></p>
              <p>
                We propose a fine-grained egocentric hand-object segmentation dataset and model, and demonstrate
                its usage to multiple downstream vision applications, such as activity recognition, hand-object 
                reconstruction, and seeing through the hands in videos. 
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="Placement()" onmouseover="Placement()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/cvpr_placement.jpg' style="width:132%;max-width:132%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580562.pdf">
                <papertitle>Learning Diverse Object Placement by Inpainting for Compositional Data Augmentation</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Tarmily Wen,
              Jie Min, 
              David Han,
              Jianbo Shi
              <br>
              <em>ECCV 2020</em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580562.pdf">arxiv</a>
              <p></p>
              <p>
                We study the problem of common sense placement of visual objects in an image. 
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="Scale_Edit()" onmouseover="Scale_Edit()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='scale_edit'><img src='images/cvpr_scale_edit_v3.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Nested_Scale-Editing_for_Conditional_Image_Synthesis_CVPR_2020_paper.pdf">
                <papertitle>Nested Scale-Editing for Conditional Image Synthesis</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Jiancong Wang*,
              Yinshuang Xu, 
              Jie Min,
              Tarmily Wen, 
              James C. Gee,
              Jianbo Shi (* equal contribution)
              <br>
              <em>CVPR 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Nested_Scale-Editing_for_Conditional_Image_Synthesis_CVPR_2020_paper.pdf">arxiv</a>
              <p></p>
              <p>
                We proposed an image synthesis approach that provides stratified navigation in the latent code space.
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="DeepBlend()" onmouseover="DeepBlend()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='blend'><img src='images/wacv_blend.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Deep_Image_Blending_WACV_2020_paper.pdf">
                <papertitle>Deep Image Blending</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Tarmily Wen,
              Jianbo Shi 
              <br>
              <em>WACV 2020</em>
              <br>
              <a href="https://arxiv.org/pdf/1910.11495.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/DeepImageBlending">code</a>
              <p></p>
              <p>
                We propose a deep optimization-based blending algorithm.
              </p>
            </td>
          </tr>                        
          
          <tr onmouseout="Outpaint()" onmouseover="Outpaint()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/wacv_outpaint.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Multimodal_Image_Outpainting_With_Regularized_Normalized_Diversification_WACV_2020_paper.pdf">
                <papertitle>Multimodal Image Outpainting with Regularized Normalized Diversification</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Jiancong Wang,
              Jianbo Shi 
              <br>
              <em>WACV 2020</em>
              <br>
              <a href="https://arxiv.org/pdf/1910.11481.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/DiverseOutpaint">code</a>
              <p></p>
              <p>
                We study the problem of generating a set of realistic and diverse backgrounds when given only a small foreground region, which we formulate as image outpainting task. 
              </p>
            </td>
          </tr>          
          
          <tr onmouseout="NeuralEmbedding()" onmouseover="NeuralEmbedding()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='nips_emb'><img src='images/nips_emb.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://ml4physicalsciences.github.io/files/NeurIPS_ML4PS_2019_119.pdf">
                <papertitle>Neural Embedding for Physical Manipulations</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Andong Cao*,
              Rui Li,
              Jianbo Shi (* equal contribution)
              <br>
              <em>Machine Learning for Physical Science Workshop, NeurIPS</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1907.06143.pdf">arxiv</a>
              <p></p>
              <p>
                Inspired by the properties of grid cells in mammalian brains, we build a generative model that enforces a normalized pairwise distance constraint between the latent space and output space to achieve data-efficient discovery of output spaces. 
              </p>
            </td>
          </tr>
         
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/penn_logo.jpg" style="width:100%;max-width:100%" alt="upenn">
            </td>
            <td width="65width:75%;%" valign="center">
              <a href="https://alliance.seas.upenn.edu/~cis581/wiki/index.php?title=CIS_581:_Computer_Vision_%26_Computational_Photography">Teaching Assistant, CIS581 Computer Vision and Computational Photography (Professor Jianbo Shi), Fall 2018, Fall 2019</a>
              <br>
              <br>
              <a href="https://sites.google.com/seas.upenn.edu/cis680fall19">Teaching Assistant, CIS680 Vision and Learning (Professor Jianbo Shi), Fall 2019</a>
              <br>
              <br>
              <a href="https://www.seas.upenn.edu/~cis519/spring2018/">Teaching Assistant, CIS519 Applied Machine Learning (Professor Dan Roth), Spring 2018</a>
              <br>
              <br>              
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Industrial Experiences</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/adobe.png" style="width:80%;max-width:80%" alt="ibm">
            </td>
            <td width="65width:75%;%" valign="center">
              Adobe Research Intern, May 2020 - Present
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ibm.jpg" style="width:100%;max-width:100%" alt="ibm">
            </td>
            <td width="65width:75%;%" valign="center">
              Machine Learning Intern, June - August 2015
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/zhenfund.jpg" style="width:100%;max-width:100%" alt="zhenfund">
            </td>
            <td width="65width:75%;%" valign="center">
              Investment Analyst Intern, August 2015 - February 2016
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

</body>

</html>

