<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-7580334-2');
  </script>

  <title>Lingzhi Zhang</title>
  
  <meta name="author" content="Lingzhi Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Lingzhi Zhang</name>
              </p> 
                  I am a Research Scientist at Adobe and the Tech Lead for multiple generations of Generative Remove features 
                  deployed in Photoshop and Lightroom ecosystems (desktop, mobile, web, etc.). I am interested in multimodal 
                  media understanding and synthesis. 
                  <br>
                  <br>
                  I received my PhD in Computer and Information Science at the University of Pennsylvania, advised by
                <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a>.
              </p>
              <p style="text-align:center">
                <a href="lingzhi1994@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/owenzlz">GitHub</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=d85t3QEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/lingzhi-zhang-30344785/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zlz.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/zlz.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <ul>
                <li><strong>[10/2025]</strong> - I presented "Project Trace Erase" (future removal technology) at Adobe MAX Sneaks 2025. <a href="https://www.youtube.com/watch?v=kFKTIkxI0Qw&vl=en">Youtube Recording</a>. </li>
                <li><strong>[10/2025]</strong> - Media coverages of "Project Trace Erase": <a href="https://www.cnet.com/tech/services-and-software/adobes-ai-makes-it-quicker-than-ever-to-edit-your-photos-i-got-an-early-look/">CNET</a>. </li>
                <li><strong>[03/2025]</strong> - <a href="https://arxiv.org/pdf/2506.05342">"Refer to Anything with Vision-Language Prompts"</a> has been accepted to ICCV 2025 </li>
                <li><strong>[04/2024]</strong> - News item placeholder text here</li>
                <li><strong>[08/2023]</strong> - Defended my Ph.D thesis!</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="PAL4VST()" onmouseover="PAL4VST()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/pal4vst.png' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2310.05590.pdf">
                <papertitle>Perceptual Artifacts Localization for Image Synthesis Tasks</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Zhengjie Xu,
              Connelly Barnes, 
              Yuqian Zhou,
              Qing Liu, 
              He Zhang,
              Zhe Lin,
              Eli Shechtman, 
              Sohrab Amirghodsi,
              Jianbo Shi
              <br>
              <em>ICCV 2023</em>
              <br>
              <a href="https://arxiv.org/pdf/2310.05590.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/PAL4VST">code</a> / 
              <a href="https://github.com/owenzlz/PAL4VST">dataset</a>
              <p></p>
              <p>
                We generalize Perceptual Artifacts Localization to ten diverse image synthesis, and shows promising accuracy. 
                We also show the effectiveness of automatic artifacts fixing and quality assessment as downstream applications. 
              </p>
            </td>
          </tr>

          <tr onmouseout="InpaintArtifacts()" onmouseover="InpaintArtifacts()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/PAL4Inpaint.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03357.pdf">
                <papertitle>Perceptual Artifacts Localization for Inpainting</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Yuqian Zhou,
              Connelly Barnes,
              Sohrab Amirghodsi,
              Zhe Lin,
              Eli Shechtman, 
              Jianbo Shi
              <br>
              <em>ECCV 2022 (Oral)</em>
              <br>
              <a href="https://arxiv.org/pdf/2208.03357.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/PAL4Inpaint">code</a> / 
              <a href="https://github.com/owenzlz/PAL4Inpaint">dataset</a>
              <p></p>
              <p>
                We formulate a new task of learning perceptual artifacts localization on the inpainted images.
                We propose a high-quality labeled dataset, successuflly train a model to localize the perceptual 
                artifacts, and demonstrate downstream practical applications to no-reference single image quality 
                assessment and iterative fill. 
              </p>
            </td>
          </tr>
 
          <tr onmouseout="SuperCAF()" onmouseover="SuperCAF()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/SuperCAF.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03552.pdf">
                <papertitle>Inpainting at Modern Camera Resolution by Guided PatchMatch with Auto-Curation</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Connelly Barnes,
              Kevin Wampler,
              Sohrab Amirghodsi,
              Eli Shechtman, 
              Zhe Lin, 
              Jianbo Shi
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://arxiv.org/pdf/2208.03552.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/SuperCAF">code</a> / 
              <a href="https://github.com/owenzlz/SuperCAF">dataset</a>
              <p></p>
              <p>
                We propose a hybrid deep learning and patch-based approach for inpainting at modern camera resolution (4K+).
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="EgoHOS()" onmouseover="EgoHOS()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/egohos.gif' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2208.03826.pdf">
                <papertitle>Fine-Grained Egocentric Hand-Object Segmentation: Dataset, Model, and Applications</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Shenghao Zhou*,
              Simon Stent,
              Jianbo Shi (* equal contribution)
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://www.seas.upenn.edu/~shzhou2/projects/eos_dataset/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.03826.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/EgoHOS">code</a> / 
              <a href="https://github.com/owenzlz/EgoHOS">dataset</a>
              <p></p>
              <p>
                We propose a fine-grained egocentric hand-object segmentation dataset and model, and demonstrate
                its usage to multiple downstream vision applications, such as activity recognition, hand-object 
                reconstruction, and seeing through the hands in videos. 
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="Placement()" onmouseover="Placement()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/cvpr_placement.jpg' style="width:132%;max-width:132%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580562.pdf">
                <papertitle>Learning Diverse Object Placement by Inpainting for Compositional Data Augmentation</papertitle> 
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Tarmily Wen,
              Jie Min, 
              David Han,
              Jianbo Shi
              <br>
              <em>ECCV 2020</em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580562.pdf">arxiv</a>
              <p></p>
              <p>
                We study the problem of common sense placement of visual objects in an image. 
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="Scale_Edit()" onmouseover="Scale_Edit()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='scale_edit'><img src='images/cvpr_scale_edit_v3.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Nested_Scale-Editing_for_Conditional_Image_Synthesis_CVPR_2020_paper.pdf">
                <papertitle>Nested Scale-Editing for Conditional Image Synthesis</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Jiancong Wang*,
              Yinshuang Xu, 
              Jie Min,
              Tarmily Wen, 
              James C. Gee,
              Jianbo Shi (* equal contribution)
              <br>
              <em>CVPR 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Nested_Scale-Editing_for_Conditional_Image_Synthesis_CVPR_2020_paper.pdf">arxiv</a>
              <p></p>
              <p>
                We proposed an image synthesis approach that provides stratified navigation in the latent code space.
              </p>
            </td>
          </tr>   
          
          <tr onmouseout="DeepBlend()" onmouseover="DeepBlend()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='blend'><img src='images/wacv_blend.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Deep_Image_Blending_WACV_2020_paper.pdf">
                <papertitle>Deep Image Blending</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Tarmily Wen,
              Jianbo Shi 
              <br>
              <em>WACV 2020</em>
              <br>
              <a href="https://arxiv.org/pdf/1910.11495.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/DeepImageBlending">code</a>
              <p></p>
              <p>
                We propose a deep optimization-based blending algorithm.
              </p>
            </td>
          </tr>                        
          
          <tr onmouseout="Outpaint()" onmouseover="Outpaint()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='outpaint'><img src='images/wacv_outpaint.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Zhang_Multimodal_Image_Outpainting_With_Regularized_Normalized_Diversification_WACV_2020_paper.pdf">
                <papertitle>Multimodal Image Outpainting with Regularized Normalized Diversification</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang</strong>, 
              Jiancong Wang,
              Jianbo Shi 
              <br>
              <em>WACV 2020</em>
              <br>
              <a href="https://arxiv.org/pdf/1910.11481.pdf">arxiv</a> / 
              <a href="https://github.com/owenzlz/DiverseOutpaint">code</a>
              <p></p>
              <p>
                We study the problem of generating a set of realistic and diverse backgrounds when given only a small foreground region, which we formulate as image outpainting task. 
              </p>
            </td>
          </tr>          
          
          <tr onmouseout="NeuralEmbedding()" onmouseover="NeuralEmbedding()">
            <td style="padding:10px;width:30%;vertical-align:top">
              <div class="one">
                <div class="two" id='nips_emb'><img src='images/nips_emb.jpg' style="width:130%;max-width:130%"></div>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:top">
              <a href="https://ml4physicalsciences.github.io/files/NeurIPS_ML4PS_2019_119.pdf">
                <papertitle>Neural Embedding for Physical Manipulations</papertitle>
              </a>
              <br>
              <strong>Lingzhi Zhang*</strong>, 
              Andong Cao*,
              Rui Li,
              Jianbo Shi (* equal contribution)
              <br>
              <em>Machine Learning for Physical Science Workshop, NeurIPS</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1907.06143.pdf">arxiv</a>
              <p></p>
              <p>
                Inspired by the properties of grid cells in mammalian brains, we build a generative model that enforces a normalized pairwise distance constraint between the latent space and output space to achieve data-efficient discovery of output spaces. 
              </p>
            </td>
          </tr>
         
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/penn_logo.jpg" style="width:100%;max-width:100%" alt="upenn">
            </td>
            <td width="65width:75%;%" valign="center">
              <a href="https://alliance.seas.upenn.edu/~cis581/wiki/index.php?title=CIS_581:_Computer_Vision_%26_Computational_Photography">Teaching Assistant, CIS581 Computer Vision and Computational Photography (Professor Jianbo Shi), Fall 2018, Fall 2019</a>
              <br>
              <br>
              <a href="https://sites.google.com/seas.upenn.edu/cis680fall19">Teaching Assistant, CIS680 Vision and Learning (Professor Jianbo Shi), Fall 2019</a>
              <br>
              <br>
              <a href="https://www.seas.upenn.edu/~cis519/spring2018/">Teaching Assistant, CIS519 Applied Machine Learning (Professor Dan Roth), Spring 2018</a>
              <br>
              <br>              
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Industrial Experiences</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/adobe.png" style="width:80%;max-width:80%" alt="ibm">
            </td>
            <td width="65width:75%;%" valign="center">
              Adobe Research Intern, May 2020 - Present
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ibm.jpg" style="width:100%;max-width:100%" alt="ibm">
            </td>
            <td width="65width:75%;%" valign="center">
              Machine Learning Intern, June - August 2015
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/zhenfund.jpg" style="width:100%;max-width:100%" alt="zhenfund">
            </td>
            <td width="65width:75%;%" valign="center">
              Investment Analyst Intern, August 2015 - February 2016
              <br>
              <br>           
            </td>
          </tr>
      </td>
    </tr>
  </table>

</body>

</html>

